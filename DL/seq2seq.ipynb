{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Seq2Seq Translation without Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-18 17:15:58--  https://www.statmt.org/europarl/v7/fr-en.tgz\r\n",
      "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\r\n",
      "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 202718517 (193M) [application/x-gzip]\r\n",
      "Saving to: ‘data/fr-en.tgz’\r\n",
      "\r\n",
      "data/fr-en.tgz       18%[==>                 ]  36.59M  4.05MB/s    eta 44s    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/IPython/utils/_process_posix.py:153\u001B[0m, in \u001B[0;36mProcessHandler.system\u001B[0;34m(self, cmd)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001B[39;00m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001B[39;00m\n\u001B[0;32m--> 153\u001B[0m     res_idx \u001B[38;5;241m=\u001B[39m \u001B[43mchild\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpect_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_timeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28mprint\u001B[39m(child\u001B[38;5;241m.\u001B[39mbefore[out_size:]\u001B[38;5;241m.\u001B[39mdecode(enc, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreplace\u001B[39m\u001B[38;5;124m'\u001B[39m), end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pexpect/spawnbase.py:372\u001B[0m, in \u001B[0;36mSpawnBase.expect_list\u001B[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001B[0m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 372\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mexp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpect_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pexpect/expect.py:169\u001B[0m, in \u001B[0;36mExpecter.expect_loop\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;66;03m# Still have time left, so read more data\u001B[39;00m\n\u001B[0;32m--> 169\u001B[0m incoming \u001B[38;5;241m=\u001B[39m \u001B[43mspawn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_nonblocking\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspawn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaxread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspawn\u001B[38;5;241m.\u001B[39mdelayafterread \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pexpect/pty_spawn.py:500\u001B[0m, in \u001B[0;36mspawn.read_nonblocking\u001B[0;34m(self, size, timeout)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;66;03m# Because of the select(0) check above, we know that no data\u001B[39;00m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001B[39;00m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001B[39;00m\n\u001B[0;32m--> 500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (timeout \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(spawn, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mread_nonblocking(size)\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pexpect/pty_spawn.py:450\u001B[0m, in \u001B[0;36mspawn.read_nonblocking.<locals>.select\u001B[0;34m(timeout)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(timeout):\n\u001B[0;32m--> 450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mselect_ignore_interrupts\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchild_fd\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pexpect/utils.py:143\u001B[0m, in \u001B[0;36mselect_ignore_interrupts\u001B[0;34m(iwtd, owtd, ewtd, timeout)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mselect\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43miwtd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mowtd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mewtd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msystem\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwget https://www.statmt.org/europarl/v7/fr-en.tgz -O data/fr-en.tgz\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/ipykernel/zmqshell.py:636\u001B[0m, in \u001B[0;36mZMQInteractiveShell.system_piped\u001B[0;34m(self, cmd)\u001B[0m\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_ns[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_exit_code\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m system(cmd)\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 636\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_ns[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_exit_code\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43msystem\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvar_expand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcmd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/IPython/utils/_process_posix.py:164\u001B[0m, in \u001B[0;36mProcessHandler.system\u001B[0;34m(self, cmd)\u001B[0m\n\u001B[1;32m    159\u001B[0m         out_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(child\u001B[38;5;241m.\u001B[39mbefore)\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001B[39;00m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# curses.ascii.ETX).\u001B[39;00m\n\u001B[0;32m--> 164\u001B[0m     \u001B[43mchild\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msendline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mchr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# Read and print any more output the program might produce on its\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;66;03m# way out.\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pexpect/pty_spawn.py:578\u001B[0m, in \u001B[0;36mspawn.sendline\u001B[0;34m(self, s)\u001B[0m\n\u001B[1;32m    572\u001B[0m \u001B[38;5;124;03m'''Wraps send(), sending string ``s`` to child process, with\u001B[39;00m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;124;03m``os.linesep`` automatically appended. Returns number of bytes\u001B[39;00m\n\u001B[1;32m    574\u001B[0m \u001B[38;5;124;03mwritten.  Only a limited number of bytes may be sent for each\u001B[39;00m\n\u001B[1;32m    575\u001B[0m \u001B[38;5;124;03mline in the default terminal mode, see docstring of :meth:`send`.\u001B[39;00m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m    577\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_coerce_send_string(s)\n\u001B[0;32m--> 578\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinesep\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pexpect/pty_spawn.py:563\u001B[0m, in \u001B[0;36mspawn.send\u001B[0;34m(self, s)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;124;03m'''Sends string ``s`` to the child process, returning the number of\u001B[39;00m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;124;03mbytes written. If a logfile is specified, a copy is written to that\u001B[39;00m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;124;03mlog.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;124;03m    >>> bash.sendline('x' * 5000)\u001B[39;00m\n\u001B[1;32m    560\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelaybeforesend \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 563\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdelaybeforesend\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    565\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_coerce_send_string(s)\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log(s, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msend\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "!wget https://www.statmt.org/europarl/v7/fr-en.tgz -O data/fr-en.tgz\n",
    "!tar -xf data/fr-en.tgz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ! pip install spacy (if not installed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English document: 2007723 sentences\n",
      "French document: 2007723 sentences\n"
     ]
    }
   ],
   "source": [
    "def load_document(file_name):\n",
    "    with open(file_name, 'rt', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def to_sentences(document):\n",
    "    return document.strip().split('\\n')\n",
    "\n",
    "en_file = 'data/europarl-v7.fr-en.en'\n",
    "fr_file = 'data/europarl-v7.fr-en.fr'\n",
    "\n",
    "en_document = load_document(en_file)\n",
    "fr_document = load_document(fr_file)\n",
    "\n",
    "print(f'English document: {len(to_sentences(en_document))} sentences')\n",
    "print(f'French document: {len(to_sentences(fr_document))} sentences')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000B\f\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.printable)\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brötchen\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "print(normalize('NFD', 'brötchen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'what brotchen means ?'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_lines(lines):\n",
    "    if isinstance(lines, list):\n",
    "        return [clean_lines(line) for line in lines]\n",
    "    \n",
    "    is_question = lines.endswith('?')\n",
    "    remove_punctuation = str.maketrans('', '', string.punctuation)\n",
    "    lines = normalize('NFD', lines).encode('ascii', 'ignore')\n",
    "    lines = lines.decode('UTF-8')\n",
    "    lines = lines.lower()\n",
    "    lines = lines.translate(remove_punctuation)\n",
    "    lines = re.sub(rf'[^{re.escape(string.printable)}]', '', lines)\n",
    "    \n",
    "    lines = [word for word in lines.split() if word.isalpha()]\n",
    "    if is_question:\n",
    "        lines.append('?')\n",
    "    return ' '.join(lines)\n",
    "\n",
    "clean_lines('What brötchen means?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English cleaned:\n",
      "French cleaned:\n"
     ]
    }
   ],
   "source": [
    "# save clean lines to file\n",
    "import pickle\n",
    "\n",
    "cleaned_en = clean_lines(to_sentences(en_document))\n",
    "cleaned_fr = clean_lines(to_sentences(fr_document))\n",
    "\n",
    "with open('data/cleaned_en.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned_en, f)\n",
    "    print('English cleaned:')\n",
    "    \n",
    "with open('data/cleaned_fr.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned_fr, f)\n",
    "    print('French cleaned:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[English] on behalf of the inddem group pl mr president the present debate on breaches of human rights is particularly shocking and repulsive as it relates to human trafficking and sexual abuse against children in liberia haiti congo and elsewhere by the staff of humanitarian missions who are supposed to provide help and care to the victims of starvation and armed conflict and ensure their security protection and nourishment\n",
      "[French] au nom du groupe inddem pl monsieur le president le present debat sur les violations des droits de lhomme est particulierement choquant et repoussant puisquil est lie au trafic detres humains et aux abus sexuels denfants au liberia en haiti au congo et ailleurs perpetres par le personnel de missions humanitaires cense apporter de laide et des soins aux victimes de la faim et des conflits armes et assurer leur securite leur protection et leur alimentation\n",
      "\n",
      "[English] question no by kirsten jensen\n",
      "[French] jappelle la question de mme kirsten jensen\n",
      "\n",
      "[English] there are no more than two posts for monitoring community environmental law which accounts for of the laws we adopt\n",
      "[French] il nexiste pas plus de deux postes pour surveiller lexecution du droit environnemental communautaire qui represente des lois que nous adoptons\n",
      "\n",
      "[English] forest biodiversity is threatened by illegal deforestation carried out globally\n",
      "[French] la biodiversite des forets est menacee par la deforestation illegale menee a lechelle planetaire\n",
      "\n",
      "[English] it is probable that these negotiations and the agreements which were then entered into also contain a clause about the circumstances in which individual states may cancel these agreements\n",
      "[French] il est raisonnable de penser que ces negociations et les accords sur lesquels elles ont debouche contiennent aussi une clause evoquant les conditions dans lesquelles les differents pays pourront resilier les accords en question\n",
      "\n",
      "[English] to remedy this the regulation needs to be supplemented by more detailed rules on national inspections\n",
      "[French] pour y remedier ce reglement a besoin detre complete par des regles plus detaillees concernant les inspections nationales\n",
      "\n",
      "[English] in the discussions on turkey we were surprised by the highly original and appropriate formulations which placed responsibility for accession fairly and squarely on turkey itself\n",
      "[French] pour ce qui est de la turquie nous avons ete surpris par les formules tres originales et appropriees qui ont clairement mis la responsabilite de ladhesion du cote de la turquie ellememe\n",
      "\n",
      "[English] mrpresident mralexander mrbarroso ladies and gentlemen the european union is facing an identity crisis a loss of its values and general disillusionment\n",
      "[French] monsieur le president monsieur le president en exercice du conseil monsieur le president de la commission mes chers collegues lunion europeenne fait face a une crise didentite a une perte de ses valeurs a un desenchantement general\n",
      "\n",
      "[English] the adoption of this report by the european parliament and the development of the plan for fighting counterfeiting and piracy on a european level will reduce this loss\n",
      "[French] ladoption de ce rapport par le parlement europeen ainsi que la mise au point du plan en faveur de la lutte contre la contrefacon et le piratage au niveau europeen vont contribuer a reduire ces pertes\n",
      "\n",
      "[English] thank you commissioner we are behind you\n",
      "[French] merci monsieur le commissaire nous sommes avec vous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in np.random.randint(0, len(cleaned_en), 10):\n",
    "    print(f'[English] {cleaned_en[i]}')\n",
    "    print(f'[French] {cleaned_fr[i]}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "you can restart the notebook if available memory is not enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3080 (8, 6)\n"
     ]
    }
   ],
   "source": [
    "print(th.cuda.get_device_name(), th.cuda.get_device_capability())\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(th.cuda.memory_summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.cuda.manual_seed(seed)\n",
    "    th.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>reprise de la session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i declare resumed the session of the european ...</td>\n",
       "      <td>je declare reprise la session du parlement eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although as you will have seen the dreaded mil...</td>\n",
       "      <td>comme vous avez pu le constater le grand bogue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you have requested a debate on this subject in...</td>\n",
       "      <td>vous avez souhaite un debat a ce sujet dans le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the meantime i should like to observe a min...</td>\n",
       "      <td>en attendant je souhaiterais comme un certain ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007718</th>\n",
       "      <td>i would also like although they are absent to ...</td>\n",
       "      <td>je me permettrai meme bien quils soient absent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007719</th>\n",
       "      <td>i am not going to reopen the millennium or not...</td>\n",
       "      <td>je ne rouvrirai pas le debat sur le millenaire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007720</th>\n",
       "      <td>adjournment of the session</td>\n",
       "      <td>interruption de la session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007721</th>\n",
       "      <td>i declare the session of the european parliame...</td>\n",
       "      <td>je declare interrompue la session du parlement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007722</th>\n",
       "      <td>the sitting was closed at am</td>\n",
       "      <td>la seance est levee a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2007723 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        en  \\\n",
       "0                                resumption of the session   \n",
       "1        i declare resumed the session of the european ...   \n",
       "2        although as you will have seen the dreaded mil...   \n",
       "3        you have requested a debate on this subject in...   \n",
       "4        in the meantime i should like to observe a min...   \n",
       "...                                                    ...   \n",
       "2007718  i would also like although they are absent to ...   \n",
       "2007719  i am not going to reopen the millennium or not...   \n",
       "2007720                         adjournment of the session   \n",
       "2007721  i declare the session of the european parliame...   \n",
       "2007722                       the sitting was closed at am   \n",
       "\n",
       "                                                        fr  \n",
       "0                                    reprise de la session  \n",
       "1        je declare reprise la session du parlement eur...  \n",
       "2        comme vous avez pu le constater le grand bogue...  \n",
       "3        vous avez souhaite un debat a ce sujet dans le...  \n",
       "4        en attendant je souhaiterais comme un certain ...  \n",
       "...                                                    ...  \n",
       "2007718  je me permettrai meme bien quils soient absent...  \n",
       "2007719  je ne rouvrirai pas le debat sur le millenaire...  \n",
       "2007720                         interruption de la session  \n",
       "2007721  je declare interrompue la session du parlement...  \n",
       "2007722                              la seance est levee a  \n",
       "\n",
       "[2007723 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cleaned_en = pd.read_pickle('data/cleaned_en.pkl')\n",
    "cleaned_fr = pd.read_pickle('data/cleaned_fr.pkl')\n",
    "\n",
    "en_fr = pd.DataFrame({'en': cleaned_en, 'fr': cleaned_fr})\n",
    "en_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select 150k samples due to memory limitation\n",
    "\n",
    "en_fr = en_fr.sample(150000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 104400\t(69.60%)\n",
      "Valid data: 15600\t(10.40%)\n",
      "Test data: 30000\t(20.00%)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(en_fr, test_size=0.2, random_state=42)\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.13, random_state=42)\n",
    "\n",
    "print(f'Train data: {len(train_df)}\\t({len(train_df) / len(en_fr) * 100:.2f}%)')\n",
    "print(f'Valid data: {len(valid_df)}\\t({len(valid_df) / len(en_fr) * 100:.2f}%)')\n",
    "print(f'Test data: {len(test_df)}\\t({len(test_df) / len(en_fr) * 100:.2f}%)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build input & label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torchtext"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 15:43:15.914360: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 15:43:16.004503: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-18 15:43:16.502728: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 15:43:16.502783: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 15:43:16.502788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 15:43:17.045267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-18 15:43:17.045485: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-18 15:43:17.066414: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-18 15:43:17.066510: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/bluesun/anaconda3/envs/nlp/lib/python3.8/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n",
      "/home/bluesun/anaconda3/envs/nlp/lib/python3.8/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"fr\" could not be loaded, trying \"fr_core_news_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 20633\n",
      "French vocab size: 28871\n"
     ]
    }
   ],
   "source": [
    "en_vocab = build_vocab_from_iterator(map(en_tokenizer, train_df['en']),\n",
    "                                     specials=['<unk>', '<pad>', '<bos>', '<eos>'],\n",
    "                                     min_freq=2)\n",
    "fr_vocab = build_vocab_from_iterator(map(fr_tokenizer, train_df['fr']),\n",
    "                                     specials=['<unk>', '<pad>', '<bos>', '<eos>'],\n",
    "                                     min_freq=2)\n",
    "\n",
    "en_vocab.set_default_index(en_vocab['<unk>'])\n",
    "fr_vocab.set_default_index(fr_vocab['<unk>'])\n",
    "\n",
    "print(f'English vocab size: {len(en_vocab)}')\n",
    "print(f'French vocab size: {len(fr_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cleand> the terms scalable and large scale have been used in machine learning circles long before there was big data\n",
      "<splited> ['the', 'terms', 'scalable', 'and', 'large', 'scale', 'have', 'been', 'used', 'in', 'machine', 'learning', 'circles', 'long', 'before', 'there', 'was', 'big', 'data']\n",
      "<encoded> [4, 255, 0, 7, 436, 1664, 22, 44, 337, 8, 5824, 2012, 5494, 308, 213, 43, 51, 1307, 467]\n",
      "<restored> the terms <unk> and large scale have been used in machine learning circles long before there was big data\n"
     ]
    }
   ],
   "source": [
    "# encoding\n",
    "txt = \"The terms “scalable” and “large scale” have been used in machine learning circles long before there was big data.\"\n",
    "txt = clean_lines(txt)\n",
    "print(f'<cleand> {txt}')\n",
    "\n",
    "tokens = en_tokenizer(txt)\n",
    "encoded = [en_vocab[token] for token in tokens]\n",
    "print(f'<splited> {tokens}')\n",
    "print(f'<encoded> {encoded}')\n",
    "\n",
    "# decoding\n",
    "decoded = [en_vocab.get_itos()[idx] for idx in encoded]\n",
    "print(f'<restored> {\" \".join(decoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104400/104400 [00:11<00:00, 9394.65it/s]\n",
      "100%|██████████| 15600/15600 [00:01<00:00, 8948.38it/s]\n",
      "100%|██████████| 30000/30000 [00:03<00:00, 9069.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def data_process(data: pd.DataFrame) -> List[Tuple[th.Tensor, th.Tensor]]:\n",
    "    tensor_output = []\n",
    "    for i in tqdm(range(len(data)), total=len(data)):\n",
    "        # skip too long sentences\n",
    "        if len(data['en'].iloc[i]) > 500 or len(data['fr'].iloc[i]) > 500:\n",
    "            continue\n",
    "        en_tensor = th.tensor([en_vocab[token] for token in en_tokenizer(data['en'].iloc[i])], dtype=th.long)\n",
    "        fr_tensor = th.tensor([fr_vocab[token] for token in fr_tokenizer(data['fr'].iloc[i])], dtype=th.long)\n",
    "        tensor_output.append((en_tensor, fr_tensor))\n",
    "    return tensor_output\n",
    "\n",
    "train_data = data_process(train_df)\n",
    "valid_data = data_process(valid_df)\n",
    "test_data = data_process(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 103583\n",
      "Valid data: 15472\n",
      "Test data: 29794\n",
      "\n",
      "Max length:\n",
      "\ten: 97\n",
      "\tfr: 89\n",
      "Mean length:\n",
      "\ten: 24.29\n",
      "\tfr: 25.32\n",
      "\n",
      "Max length:\n",
      "\ten: 85\n",
      "\tfr: 90\n",
      "Mean length:\n",
      "\ten: 24.01\n",
      "\tfr: 24.99\n",
      "\n",
      "Max length:\n",
      "\ten: 90\n",
      "\tfr: 90\n",
      "Mean length:\n",
      "\ten: 24.35\n",
      "\tfr: 25.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Train data: {len(train_data)}')\n",
    "print(f'Valid data: {len(valid_data)}')\n",
    "print(f'Test data: {len(test_data)}\\n')\n",
    "\n",
    "for data in [train_data, valid_data, test_data]:\n",
    "    print(f'Max length:')\n",
    "    print(f'\\ten: {max([len(en) for en, fr in data])}')\n",
    "    print(f'\\tfr: {max([len(fr) for en, fr in data])}')\n",
    "    print(f'Mean length:')\n",
    "    print(f'\\ten: {np.mean([len(en) for en, fr in data]):.2f}')\n",
    "    print(f'\\tfr: {np.mean([len(fr) for en, fr in data]):.2f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved.\n",
      "Valid data saved.\n",
      "Test data saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "    print('Train data saved.')\n",
    "\n",
    "with open('data/valid_data.pkl', 'wb') as f:\n",
    "    pickle.dump(valid_data, f)\n",
    "    print('Valid data saved.')\n",
    "\n",
    "with open('data/test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f)\n",
    "    print('Test data saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary saved.\n",
      "French vocabulary saved.\n"
     ]
    }
   ],
   "source": [
    "with open('data/en_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(en_vocab, f)\n",
    "    print('English vocabulary saved.')\n",
    "\n",
    "with open('data/fr_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(fr_vocab, f)\n",
    "    print('French vocabulary saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "# device = th.device('cpu')\n",
    "\n",
    "en_vocab = pd.read_pickle('data/en_vocab.pkl')\n",
    "fr_vocab = pd.read_pickle('data/fr_vocab.pkl')\n",
    "\n",
    "train_data = pd.read_pickle('data/train_data.pkl')\n",
    "valid_data = pd.read_pickle('data/valid_data.pkl')\n",
    "test_data = pd.read_pickle('data/test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(103583, 15472, 29794)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "batch_size = 148\n",
    "PAD_IDX = en_vocab['<pad>']\n",
    "SOS_IDX = en_vocab['<bos>']\n",
    "EOS_IDX = en_vocab['<eos>']\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    en_batch, fr_batch = [], []\n",
    "    for (en_item, fr_item) in data_batch:\n",
    "        en_batch.append(th.cat([th.tensor([SOS_IDX]), en_item, th.tensor([EOS_IDX])], dim=0))\n",
    "        fr_batch.append(th.cat([th.tensor([SOS_IDX]), fr_item, th.tensor([EOS_IDX])], dim=0))\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return en_batch, fr_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,   29,   42,  ...,    1,    1,    1],\n",
      "        [   2,   17,   11,  ...,    1,    1,    1],\n",
      "        [   2,   13,   26,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,   63,  223,  ...,    1,    1,    1],\n",
      "        [   2,   17, 1441,  ...,    1,    1,    1],\n",
      "        [   2,  647,    4,  ...,    1,    1,    1]])\n",
      "torch.Size([64, 56])\n"
     ]
    }
   ],
   "source": [
    "iter_ = iter(train_iter)\n",
    "en_batch, fr_batch = next(iter_)\n",
    "print(en_batch)\n",
    "print(en_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> mr president ladies and gentlemen i am glad to have this first opportunity to speak to the house <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<bos> monsieur le president mesdames et messieurs les deputes je me rejouis davoir la possibilite de mexprimer pour la premiere fois devant cette haute assemblee <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(en_vocab.get_itos()[idx] for idx in en_batch[0]))\n",
    "print(' '.join(fr_vocab.get_itos()[idx] for idx in fr_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, input_dim:int, emb_dim:int, encoder_hid_dim:int, decoder_hid_dim:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, encoder_hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(encoder_hid_dim * 2, decoder_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src:th.Tensor):\n",
    "        \"\"\"\n",
    "        src: [seq_len, batch_size]\n",
    "        output: [seq_len, batch_size, 2 * encoder_hid_dim], [batch_size, decoder_hid_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(src))    # [bs, seq_len, emb_dim]\n",
    "        # outputs: [seq_len, batch_size, encoder_hid_dim * 2], hidden: [2, batch_size, encoder_hid_dim]\n",
    "        outputs, hidden = self.rnn(embedded)    # [bs, seq_len, 2 * enc_hid], [2, bs, enc_hid]\n",
    "        # fc input: concat last forward and backward hidden state\n",
    "        hidden = th.tanh(self.fc(th.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=-1))) # [bs, dec_hid]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 68, 512]), torch.Size([64, 256]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = SimpleEncoder(len(en_vocab), 128, 256, 256, 0.5)\n",
    "a, b = tmp(en_batch)\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hid_dim: int, decoder_hid_dim: int, attn_dim: int):\n",
    "        super().__init__()\n",
    "        self.attn_input_dim = encoder_hid_dim * 2 + decoder_hid_dim\n",
    "        self.attn = nn.Linear(self.attn_input_dim, attn_dim)\n",
    "\n",
    "    def forward(self, decoder_hidden: th.Tensor, encoder_outputs: th.Tensor):\n",
    "        \"\"\"\n",
    "        decoder_hidden: [batch_size, decoder_hid_dim]\n",
    "        encoder_outputs: [batch_size, seq_len, 2 * encoder_hid_dim]\n",
    "        output: [batch_size, seq_len]\n",
    "\n",
    "        this is a concat attention\n",
    "        \"\"\"\n",
    "        src_len = encoder_outputs.shape[1]  # 1 for batch_first else 0\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1) # [bs, seq_len, dec_hid]\n",
    "        energy = self.attn(th.cat((repeated_decoder_hidden, encoder_outputs), dim=-1))  # [bs, seq_len, attn_dim]\n",
    "        attention = energy.tanh().sum(dim=-1)   # [bs, seq_len]\n",
    "        return F.softmax(attention, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 68, 256]) torch.Size([64, 68, 512])\n",
      "torch.Size([64, 68, 256])\n",
      "torch.Size([64, 68])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 68])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = Attention(256, 256, 256)\n",
    "tmp(b, a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, output_dim:int, emb_dim:int, encoder_hid_dim:int, decoder_hid_dim:int, dropout:float, attention:Attention):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim + encoder_hid_dim * 2, decoder_hid_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(attention.attn_input_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _weighted_encoder_outputs(self, decoder_hidden: th.Tensor, encoder_outputs: th.Tensor):\n",
    "        \"\"\"\n",
    "        decoder_hidden: [batch_size, decoder_hid_dim]\n",
    "        encoder_outputs: [batch_size, seq_len, 2 * encoder_hid_dim]\n",
    "        output: [batch_size, 1, 2 * encoder_hid_dim]\n",
    "        \"\"\"\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)  # [bs, seq_len]\n",
    "        a = a.unsqueeze(1)\n",
    "        weighted_encoder_outputs = th.bmm(a, encoder_outputs)   # [bs, 1, 2 * enc_hid]\n",
    "        return weighted_encoder_outputs\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_inputs:th.Tensor,\n",
    "                decoder_hidden:th.Tensor,\n",
    "                encoder_outputs:th.Tensor):\n",
    "        \"\"\"\n",
    "        decoder_inputs: [seq_len, batch_size]\n",
    "        hidden: [batch_size, decoder_hid_dim]\n",
    "        encoder_outputs: [seq_len, batch_size, encoder_hid_dim * 2]\n",
    "        output: [batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        decoder_inputs = decoder_inputs.unsqueeze(1)  # [bs, 1]\n",
    "        embedded = self.dropout(self.embedding(decoder_inputs)) # [bs, 1, emb_dim]\n",
    "        # output: [batch_size, 1, encoder_hid_dim], hidden: [1, batch_size, decoder_hid_dim]\n",
    "        w_encoder_outputs = self._weighted_encoder_outputs(decoder_hidden, encoder_outputs) # [bs, 1, 2 * enc_hid]\n",
    "        rnn_input = th.cat((embedded, w_encoder_outputs), dim=-1)   # [bs, 1, emb_dim + 2 * enc_hid]\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))   # [bs, 1, dec_hid], [1, bs, dec_hid]\n",
    "        embedded = embedded.squeeze(1)  # [bs, emb_dim]\n",
    "        output = output.squeeze(1)  # [bs, dec_hid]\n",
    "        w_encoder_outputs = w_encoder_outputs.squeeze(1)    # [bs, 2 * enc_hid]\n",
    "        output = self.fc(th.cat((output, w_encoder_outputs, embedded), dim=-1)) # [bs, output_dim]\n",
    "        # output: [batch_size, output_dim]\n",
    "        return output, decoder_hidden.squeeze(0)    # [bs, output_dim], [bs, dec_hid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=896, out_features=28871, bias=True)\n",
      "28871\n",
      "torch.Size([64, 1, 68]) torch.Size([64, 68, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 28871]), torch.Size([64, 256]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = Attention(256, 256, 192)\n",
    "tmp = SimpleDecoder(len(fr_vocab), 128, 256, 256, 0.5, attn)\n",
    "print(tmp.fc)\n",
    "print(tmp.output_dim)\n",
    "x, y = tmp(en_batch[:, 0], b, a)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder:nn.Module, decoder:nn.Module, device:th.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src:th.Tensor, trg:th.Tensor, teacher_forcing_ratio:float=0.5):\n",
    "        \"\"\"\n",
    "        src: [seq_len, batch_size]\n",
    "        trg: [seq_len, batch_size]\n",
    "        output: [seq_len, batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        max_len = trg.shape[1]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        outputs = th.zeros(batch_size, max_len, output_dim).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        decoder_input = trg[:, 0]    # start with <sos>\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(decoder_input, hidden, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "            do_teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "            # select one of the ground truth or the predicted word (auto-regressive)\n",
    "            decoder_input = trg[:, t] if do_teacher_force else output.argmax(-1)\n",
    "        return outputs\n",
    "\n",
    "    def translate(self, src: th.Tensor, max_len: int = 100) -> th.Tensor:\n",
    "        with th.no_grad():\n",
    "            outputs = th.zeros(1, max_len).to(self.device).long()\n",
    "            encoder_outputs, hidden = self.encoder(src)\n",
    "            decoder_input = th.tensor([SOS_IDX] * batch_size).to(self.device)\n",
    "            for t in range(1, max_len):\n",
    "                output, hidden = self.decoder(decoder_input, hidden, encoder_outputs)\n",
    "                outputs[:, t] = output.argmax(-1)\n",
    "                decoder_input = output.argmax(-1)\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 56]) torch.Size([64, 58])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(en_batch\u001B[38;5;241m.\u001B[39mshape, fr_batch\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m----> 2\u001B[0m attn \u001B[38;5;241m=\u001B[39m \u001B[43mAttention\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m192\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m enc \u001B[38;5;241m=\u001B[39m SimpleEncoder(\u001B[38;5;28mlen\u001B[39m(en_vocab), \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m0.5\u001B[39m)\n\u001B[1;32m      4\u001B[0m dec \u001B[38;5;241m=\u001B[39m SimpleDecoder(\u001B[38;5;28mlen\u001B[39m(fr_vocab), \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, attn)\n",
      "Cell \u001B[0;32mIn[13], line 3\u001B[0m, in \u001B[0;36mAttention.__init__\u001B[0;34m(self, encoder_hid_dim, decoder_hid_dim, attn_dim)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, encoder_hid_dim: \u001B[38;5;28mint\u001B[39m, decoder_hid_dim: \u001B[38;5;28mint\u001B[39m, attn_dim: \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_input_dim \u001B[38;5;241m=\u001B[39m encoder_hid_dim \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m decoder_hid_dim\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_input_dim, attn_dim)\n",
      "Cell \u001B[0;32mIn[13], line 3\u001B[0m, in \u001B[0;36mAttention.__init__\u001B[0;34m(self, encoder_hid_dim, decoder_hid_dim, attn_dim)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, encoder_hid_dim: \u001B[38;5;28mint\u001B[39m, decoder_hid_dim: \u001B[38;5;28mint\u001B[39m, attn_dim: \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_input_dim \u001B[38;5;241m=\u001B[39m encoder_hid_dim \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m decoder_hid_dim\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_input_dim, attn_dim)\n",
      "File \u001B[0;32m/snap/pycharm-professional/319/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:880\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    877\u001B[0m             stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m plugin_stop:\n\u001B[0;32m--> 880\u001B[0m     stopped_on_plugin \u001B[38;5;241m=\u001B[39m \u001B[43mplugin_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain_debugger\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_cmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m stop:\n\u001B[1;32m    882\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_line:\n",
      "File \u001B[0;32m/snap/pycharm-professional/319/plugins/python/helpers-pro/jupyter_debug/pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[0;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[1;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[0;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/snap/pycharm-professional/319/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/snap/pycharm-professional/319/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(en_batch.shape, fr_batch.shape)\n",
    "attn = Attention(256, 256, 192)\n",
    "enc = SimpleEncoder(len(en_vocab), 128, 256, 256, 0.5)\n",
    "dec = SimpleDecoder(len(fr_vocab), 128, 256, 256, 0.5, attn)\n",
    "tmp = Seq2Seq(enc, dec, th.device('cpu'))\n",
    "x = tmp(en_batch, fr_batch)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Seq2Seq(\n  (encoder): SimpleEncoder(\n    (embedding): Embedding(28871, 64)\n    (rnn): GRU(64, 128, batch_first=True, bidirectional=True)\n    (fc): Linear(in_features=256, out_features=128, bias=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (decoder): SimpleDecoder(\n    (attention): Attention(\n      (attn): Linear(in_features=384, out_features=16, bias=True)\n    )\n    (embedding): Embedding(20633, 64)\n    (rnn): GRU(320, 128, batch_first=True)\n    (fc): Linear(in_features=448, out_features=20633, bias=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_lang = 'en'\n",
    "\n",
    "input_dim = len(fr_vocab) if to_lang == 'en' else len(en_vocab)\n",
    "output_dim = len(en_vocab) if to_lang == 'en' else len(fr_vocab)\n",
    "enc_emb_dim = 64\n",
    "dec_emb_dim = 64\n",
    "enc_hid_dim = 128\n",
    "dec_hid_dim = 128\n",
    "attn_dim = 16\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "attention = Attention(enc_hid_dim, dec_hid_dim, attn_dim)\n",
    "encoder = SimpleEncoder(input_dim, enc_emb_dim, enc_hid_dim, dec_hid_dim, enc_dropout)\n",
    "decoder = SimpleDecoder(output_dim, dec_emb_dim, enc_hid_dim, dec_hid_dim, dec_dropout, attention)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = th.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab['<pad>'] if to_lang == 'en' else fr_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "valid_text = \"Il vaut mieux voir une fois que d'entendre mille fois.\"\n",
    "valid_text = fr_tokenizer(clean_lines(valid_text))\n",
    "valid_text = [fr_vocab['<bos>']] + [fr_vocab[token] for token in valid_text] + [fr_vocab['<eos>']]\n",
    "valid_text = th.tensor(valid_text).unsqueeze(0).to(device)\n",
    "valid_label = \"It is better to see once than to hear a thousand times.\"\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          dataloader: DataLoader,\n",
    "          optimizer: th.optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float,\n",
    "          to_lang='en'):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False, total=len(dataloader))\n",
    "    for i, (src, trg) in enumerate(progress_bar):\n",
    "        if to_lang == 'en':\n",
    "            src, trg = trg, src\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        th.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            gc.collect()\n",
    "        th.cuda.empty_cache()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             dataloader: DataLoader,\n",
    "             criterion: nn.Module,\n",
    "             to_lang='en'):\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with th.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc='Evaluating', leave=False, total=len(dataloader))\n",
    "        for src, trg in progress_bar:\n",
    "            if to_lang == 'en':\n",
    "                src, trg = trg, src\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            output = model(src, trg, teacher_forcing_ratio=0) # turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        output = model.translate(valid_text, to_lang)\n",
    "        output = output.squeeze(0)\n",
    "        output = ' '.join([en_vocab.get_itos()[idx] for idx in output])\n",
    "        print(f'Predicted: {output}')\n",
    "\n",
    "        th.cuda.empty_cache()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run(model: nn.Module,\n",
    "        n_epochs: int,\n",
    "        train_dataloader: DataLoader,\n",
    "        valid_dataloader: DataLoader,\n",
    "        optimizer: th.optim.Optimizer,\n",
    "        criterion: nn.Module,\n",
    "        clip: float,\n",
    "        to_lang='en'):\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_dataloader, optimizer, criterion, clip, to_lang)\n",
    "        valid_loss = evaluate(model, valid_dataloader, criterion, to_lang)\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            th.save(model.state_dict(), 'seq2seq-model.pt')\n",
    "            print('Best model saved')\n",
    "        print(f'Epoch: {epoch+1:02}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        th.cuda.empty_cache()\n",
    "\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 1.28 GB\n"
     ]
    }
   ],
   "source": [
    "# memory usage\n",
    "print(f'Memory Usage: {th.cuda.memory_allocated() / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:\t<bos> il vaut mieux voir une fois que dentendre mille fois <eos>\n",
      "pred:\tit is not that the that is not to the to the to the to the <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "trg:\tit is better to see once than to hear a thousand times\n",
      "\n",
      "Epoch: 01\n",
      "\tTrain Loss: 6.143\n",
      "\t Val. Loss: 6.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:\t<bos> il vaut mieux voir une fois que dentendre mille fois <eos>\n",
      "pred:\tit is more than to the that i have been to to to to <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "trg:\tit is better to see once than to hear a thousand times\n",
      "\n",
      "Epoch: 02\n",
      "\tTrain Loss: 5.354\n",
      "\t Val. Loss: 5.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:\t<bos> il vaut mieux voir une fois que dentendre mille fois <eos>\n",
      "pred:\tit is better better than to the time to speak with the time <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "trg:\tit is better to see once than to hear a thousand times\n",
      "\n",
      "Epoch: 03\n",
      "\tTrain Loss: 4.760\n",
      "\t Val. Loss: 5.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:\t<bos> il vaut mieux voir une fois que dentendre mille fois <eos>\n",
      "pred:\tit is better better than once again once again once again once again <eos> once again <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n",
      "trg:\tit is better to see once than to hear a thousand times\n",
      "\n",
      "Epoch: 04\n",
      "\tTrain Loss: 4.367\n",
      "\t Val. Loss: 5.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:\t<bos> il vaut mieux voir une fois que dentendre mille fois <eos>\n",
      "pred:\tit is better better than once again once again once again once again once again once again <eos> once again <eos> once again <eos> once again <eos> once again <eos> once again <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> once again <eos> once again <eos> once again <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> once again <eos> once again <eos> once again <eos> <eos> <eos> <eos> <eos> <eos> <eos> once again <eos> once again <eos> once again <eos> <eos> <eos> <eos> <eos> <eos> <eos> once again <eos> once again <eos> once again <eos> <eos> <eos> <eos> <eos>\n",
      "trg:\tit is better to see once than to hear a thousand times\n",
      "\n",
      "Epoch: 05\n",
      "\tTrain Loss: 4.117\n",
      "\t Val. Loss: 5.157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 588/785 [06:01<02:04,  1.58it/s, loss=3.74]"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> i am a supporter of the <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(model: nn.Module, input_text: str):\n",
    "    model.eval()\n",
    "    tokenizer = fr_tokenizer if to_lang == 'en' else en_tokenizer\n",
    "    if to_lang == 'en':\n",
    "        from_vocab, to_vocab = fr_vocab, en_vocab\n",
    "    else:\n",
    "        from_vocab, to_vocab = en_vocab, fr_vocab\n",
    "\n",
    "    input_text = tokenizer(clean_lines(input_text))\n",
    "    input_text = [from_vocab['<bos>']] + [from_vocab[token] for token in input_text] + [from_vocab['<eos>']]\n",
    "    input_text = th.tensor(input_text).unsqueeze(0).to(device)\n",
    "    output = model.translate(input_text)\n",
    "    output = ' '.join(to_vocab.get_itos()[idx] for idx in output.squeeze(0))\n",
    "    return output\n",
    "\n",
    "model.load_state_dict(th.load('seq2seq-model.pt'))\n",
    "translate(model, 'Je suis un étudiant.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c99887877cf8154d7844f7dd912fc6fe217beebf8c55bc48bb11321ac59cf8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
