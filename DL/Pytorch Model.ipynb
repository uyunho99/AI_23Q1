{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (Dense_Layer): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self): # class 초기화\n",
    "        super().__init__() # 부모 클래스(nn.Module의 __init__()) 상속\n",
    "        dense = nn.Linear(10, 10) # dense layer: 10개의 input, 10개의 output\n",
    "        nn.init.kaiming_normal_(dense.weight) # weight 초기화(He 초기화: ReLU 활성화 함수 사용 시)\n",
    "        self.Dense_Layer = nn.Sequential(dense, nn.ReLU()) # sequential: 순차적으로 층을 쌓음(dense layer, ReLU(activation)))\n",
    "    \n",
    "    def forward(self, x): # forward propagation\n",
    "        return self.Dense_Layer(x) # Dense_Layer에 x를 넣어서 결과를 반환\n",
    "    \n",
    "print(MyModel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수형 API(like tensorflow/keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (extractor1): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (extractor2): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=200, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (3): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size = 64, activation = nn.ReLU):\n",
    "        super(MyModel, self).__init__() # 부모 클래스(nn.Module의 __init__()) 상속\n",
    "        # sequential: 순차적으로 층을 쌓음\n",
    "        self.extractor1 = nn.Sequential(nn.Linear(input_size, 300),\n",
    "                                                    activation(),\n",
    "                                                    nn.Linear(300, 100),\n",
    "                                                    activation())\n",
    "        \n",
    "        self.extractor2 = nn.Sequential(nn.Linear(input_size, 200),\n",
    "                                                    activation())\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(300, 100),\n",
    "                                        activation(),\n",
    "                                        nn.Linear(100, 10),\n",
    "                                        nn.Softmax())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x1 = self.extractor1(inputs) # extractor1에 inputs를 넣어서 결과를 반환\n",
    "        x2 = self.extractor2(inputs) # extractor2에 inputs를 넣어서 결과를 반환\n",
    "        x = torch.cat((x1, x2), dim = 1) # x1과 x2를 합침\n",
    "        x = self.classifier(x) # classifier에 x를 넣어서 결과를 반환\n",
    "        return x\n",
    "    \n",
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunho/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 100]                  --\n",
       "│    └─Linear: 2-1                       [1, 300]                  19,500\n",
       "│    └─ReLU: 2-2                         [1, 300]                  --\n",
       "│    └─Linear: 2-3                       [1, 100]                  30,100\n",
       "│    └─ReLU: 2-4                         [1, 100]                  --\n",
       "├─Sequential: 1-2                        [1, 200]                  --\n",
       "│    └─Linear: 2-5                       [1, 200]                  13,000\n",
       "│    └─ReLU: 2-6                         [1, 200]                  --\n",
       "├─Sequential: 1-3                        [1, 10]                   --\n",
       "│    └─Linear: 2-7                       [1, 100]                  30,100\n",
       "│    └─ReLU: 2-8                         [1, 100]                  --\n",
       "│    └─Linear: 2-9                       [1, 10]                   1,010\n",
       "│    └─Softmax: 2-10                     [1, 10]                   --\n",
       "==========================================================================================\n",
       "Total params: 93,710\n",
       "Trainable params: 93,710\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.09\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.37\n",
       "Estimated Total Size (MB): 0.38\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size = (1, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"674pt\" height=\"787pt\"\n viewBox=\"0.00 0.00 674.00 787.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 783)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-783 670,-783 670,4 -4,4\"/>\n<!-- 5214856672 -->\n<g id=\"node1\" class=\"node\">\n<title>5214856672</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"436,-31 371,-31 371,0 436,0 436,-31\"/>\n<text text-anchor=\"middle\" x=\"403.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (1, 10)</text>\n</g>\n<!-- 5214844624 -->\n<g id=\"node2\" class=\"node\">\n<title>5214844624</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"460,-86 347,-86 347,-67 460,-67 460,-86\"/>\n<text text-anchor=\"middle\" x=\"403.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">SoftmaxBackward0</text>\n</g>\n<!-- 5214844624&#45;&gt;5214856672 -->\n<g id=\"edge36\" class=\"edge\">\n<title>5214844624&#45;&gt;5214856672</title>\n<path fill=\"none\" stroke=\"black\" d=\"M403.5,-66.79C403.5,-60.07 403.5,-50.4 403.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"407,-41.19 403.5,-31.19 400,-41.19 407,-41.19\"/>\n</g>\n<!-- 5214844720 -->\n<g id=\"node3\" class=\"node\">\n<title>5214844720</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"454,-141 353,-141 353,-122 454,-122 454,-141\"/>\n<text text-anchor=\"middle\" x=\"403.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 5214844720&#45;&gt;5214844624 -->\n<g id=\"edge1\" class=\"edge\">\n<title>5214844720&#45;&gt;5214844624</title>\n<path fill=\"none\" stroke=\"black\" d=\"M403.5,-121.75C403.5,-114.8 403.5,-104.85 403.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"407,-96.09 403.5,-86.09 400,-96.09 407,-96.09\"/>\n</g>\n<!-- 5214844480 -->\n<g id=\"node4\" class=\"node\">\n<title>5214844480</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"332,-196 231,-196 231,-177 332,-177 332,-196\"/>\n<text text-anchor=\"middle\" x=\"281.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214844480&#45;&gt;5214844720 -->\n<g id=\"edge2\" class=\"edge\">\n<title>5214844480&#45;&gt;5214844720</title>\n<path fill=\"none\" stroke=\"black\" d=\"M301.1,-176.98C320.82,-168.42 351.45,-155.11 374.08,-145.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"375.76,-148.37 383.54,-141.17 372.98,-141.94 375.76,-148.37\"/>\n</g>\n<!-- 5213691632 -->\n<g id=\"node5\" class=\"node\">\n<title>5213691632</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"335,-262 216,-262 216,-232 335,-232 335,-262\"/>\n<text text-anchor=\"middle\" x=\"275.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">classifier.2.bias</text>\n<text text-anchor=\"middle\" x=\"275.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (10)</text>\n</g>\n<!-- 5213691632&#45;&gt;5214844480 -->\n<g id=\"edge3\" class=\"edge\">\n<title>5213691632&#45;&gt;5214844480</title>\n<path fill=\"none\" stroke=\"black\" d=\"M276.95,-231.84C277.73,-224.21 278.71,-214.7 279.56,-206.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"283.06,-206.57 280.6,-196.27 276.1,-205.86 283.06,-206.57\"/>\n</g>\n<!-- 5214844336 -->\n<g id=\"node6\" class=\"node\">\n<title>5214844336</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"451,-196 356,-196 356,-177 451,-177 451,-196\"/>\n<text text-anchor=\"middle\" x=\"403.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 5214844336&#45;&gt;5214844720 -->\n<g id=\"edge4\" class=\"edge\">\n<title>5214844336&#45;&gt;5214844720</title>\n<path fill=\"none\" stroke=\"black\" d=\"M403.5,-176.75C403.5,-169.8 403.5,-159.85 403.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"407,-151.09 403.5,-141.09 400,-151.09 407,-151.09\"/>\n</g>\n<!-- 5214844768 -->\n<g id=\"node7\" class=\"node\">\n<title>5214844768</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"454,-256.5 353,-256.5 353,-237.5 454,-237.5 454,-256.5\"/>\n<text text-anchor=\"middle\" x=\"403.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 5214844768&#45;&gt;5214844336 -->\n<g id=\"edge5\" class=\"edge\">\n<title>5214844768&#45;&gt;5214844336</title>\n<path fill=\"none\" stroke=\"black\" d=\"M403.5,-237.37C403.5,-229.25 403.5,-216.81 403.5,-206.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"407,-206.17 403.5,-196.17 400,-206.17 407,-206.17\"/>\n</g>\n<!-- 5214844960 -->\n<g id=\"node8\" class=\"node\">\n<title>5214844960</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"253,-322.5 152,-322.5 152,-303.5 253,-303.5 253,-322.5\"/>\n<text text-anchor=\"middle\" x=\"202.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214844960&#45;&gt;5214844768 -->\n<g id=\"edge6\" class=\"edge\">\n<title>5214844960&#45;&gt;5214844768</title>\n<path fill=\"none\" stroke=\"black\" d=\"M229.61,-303.37C265.07,-292.08 327.34,-272.25 366.95,-259.64\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"368.03,-262.97 376.5,-256.6 365.91,-256.3 368.03,-262.97\"/>\n</g>\n<!-- 5213691472 -->\n<g id=\"node9\" class=\"node\">\n<title>5213691472</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"256,-394 137,-394 137,-364 256,-364 256,-394\"/>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\">classifier.0.bias</text>\n<text text-anchor=\"middle\" x=\"196.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\"> (100)</text>\n</g>\n<!-- 5213691472&#45;&gt;5214844960 -->\n<g id=\"edge7\" class=\"edge\">\n<title>5213691472&#45;&gt;5214844960</title>\n<path fill=\"none\" stroke=\"black\" d=\"M197.83,-363.8C198.68,-354.7 199.8,-342.79 200.73,-332.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"204.22,-333.13 201.67,-322.84 197.25,-332.47 204.22,-333.13\"/>\n</g>\n<!-- 5214844912 -->\n<g id=\"node10\" class=\"node\">\n<title>5214844912</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"422,-322.5 333,-322.5 333,-303.5 422,-303.5 422,-322.5\"/>\n<text text-anchor=\"middle\" x=\"377.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">CatBackward0</text>\n</g>\n<!-- 5214844912&#45;&gt;5214844768 -->\n<g id=\"edge8\" class=\"edge\">\n<title>5214844912&#45;&gt;5214844768</title>\n<path fill=\"none\" stroke=\"black\" d=\"M381.01,-303.37C384.82,-293.97 391.04,-278.67 395.97,-266.53\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"399.36,-267.49 399.88,-256.91 392.88,-264.85 399.36,-267.49\"/>\n</g>\n<!-- 5214845056 -->\n<g id=\"node11\" class=\"node\">\n<title>5214845056</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"369,-388.5 274,-388.5 274,-369.5 369,-369.5 369,-388.5\"/>\n<text text-anchor=\"middle\" x=\"321.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 5214845056&#45;&gt;5214844912 -->\n<g id=\"edge9\" class=\"edge\">\n<title>5214845056&#45;&gt;5214844912</title>\n<path fill=\"none\" stroke=\"black\" d=\"M329.05,-369.37C337.69,-359.5 352.03,-343.11 362.87,-330.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"365.76,-332.74 369.71,-322.91 360.49,-328.13 365.76,-332.74\"/>\n</g>\n<!-- 5214845296 -->\n<g id=\"node12\" class=\"node\">\n<title>5214845296</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"296,-454.5 195,-454.5 195,-435.5 296,-435.5 296,-454.5\"/>\n<text text-anchor=\"middle\" x=\"245.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 5214845296&#45;&gt;5214845056 -->\n<g id=\"edge10\" class=\"edge\">\n<title>5214845296&#45;&gt;5214845056</title>\n<path fill=\"none\" stroke=\"black\" d=\"M255.75,-435.37C267.81,-425.21 288.06,-408.16 302.91,-395.65\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"305.53,-398.02 310.93,-388.91 301.02,-392.67 305.53,-398.02\"/>\n</g>\n<!-- 5214845440 -->\n<g id=\"node13\" class=\"node\">\n<title>5214845440</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"114,-515 13,-515 13,-496 114,-496 114,-515\"/>\n<text text-anchor=\"middle\" x=\"63.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214845440&#45;&gt;5214845296 -->\n<g id=\"edge11\" class=\"edge\">\n<title>5214845440&#45;&gt;5214845296</title>\n<path fill=\"none\" stroke=\"black\" d=\"M89.96,-495.99C121.37,-485.9 173.9,-469.01 209.12,-457.69\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"210.55,-460.91 219,-454.52 208.41,-454.24 210.55,-460.91\"/>\n</g>\n<!-- 5213691232 -->\n<g id=\"node14\" class=\"node\">\n<title>5213691232</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"119,-581 0,-581 0,-551 119,-551 119,-581\"/>\n<text text-anchor=\"middle\" x=\"59.5\" y=\"-569\" font-family=\"monospace\" font-size=\"10.00\">extractor1.2.bias</text>\n<text text-anchor=\"middle\" x=\"59.5\" y=\"-558\" font-family=\"monospace\" font-size=\"10.00\"> (100)</text>\n</g>\n<!-- 5213691232&#45;&gt;5214845440 -->\n<g id=\"edge12\" class=\"edge\">\n<title>5213691232&#45;&gt;5214845440</title>\n<path fill=\"none\" stroke=\"black\" d=\"M60.47,-550.84C60.99,-543.21 61.64,-533.7 62.2,-525.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"65.71,-525.48 62.9,-515.27 58.73,-525 65.71,-525.48\"/>\n</g>\n<!-- 5214845392 -->\n<g id=\"node15\" class=\"node\">\n<title>5214845392</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"240,-515 145,-515 145,-496 240,-496 240,-515\"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 5214845392&#45;&gt;5214845296 -->\n<g id=\"edge13\" class=\"edge\">\n<title>5214845392&#45;&gt;5214845296</title>\n<path fill=\"none\" stroke=\"black\" d=\"M200.32,-495.87C208.31,-487.05 220.89,-473.16 230.76,-462.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"233.53,-464.43 237.65,-454.67 228.34,-459.73 233.53,-464.43\"/>\n</g>\n<!-- 5214845536 -->\n<g id=\"node16\" class=\"node\">\n<title>5214845536</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"238,-575.5 137,-575.5 137,-556.5 238,-556.5 238,-575.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-563.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 5214845536&#45;&gt;5214845392 -->\n<g id=\"edge14\" class=\"edge\">\n<title>5214845536&#45;&gt;5214845392</title>\n<path fill=\"none\" stroke=\"black\" d=\"M188.24,-556.37C188.93,-548.25 190,-535.81 190.89,-525.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.39,-525.43 191.76,-515.17 187.42,-524.83 194.39,-525.43\"/>\n</g>\n<!-- 5214845728 -->\n<g id=\"node17\" class=\"node\">\n<title>5214845728</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"121,-641.5 20,-641.5 20,-622.5 121,-622.5 121,-641.5\"/>\n<text text-anchor=\"middle\" x=\"70.5\" y=\"-629.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214845728&#45;&gt;5214845536 -->\n<g id=\"edge15\" class=\"edge\">\n<title>5214845728&#45;&gt;5214845536</title>\n<path fill=\"none\" stroke=\"black\" d=\"M86.28,-622.37C105.91,-611.63 139.64,-593.18 162.73,-580.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"164.5,-583.57 171.6,-575.7 161.14,-577.43 164.5,-583.57\"/>\n</g>\n<!-- 5213691312 -->\n<g id=\"node18\" class=\"node\">\n<title>5213691312</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"125,-713 6,-713 6,-683 125,-683 125,-713\"/>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-701\" font-family=\"monospace\" font-size=\"10.00\">extractor1.0.bias</text>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-690\" font-family=\"monospace\" font-size=\"10.00\"> (300)</text>\n</g>\n<!-- 5213691312&#45;&gt;5214845728 -->\n<g id=\"edge16\" class=\"edge\">\n<title>5213691312&#45;&gt;5214845728</title>\n<path fill=\"none\" stroke=\"black\" d=\"M66.61,-682.8C67.32,-673.7 68.25,-661.79 69.02,-651.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"72.52,-652.09 69.81,-641.84 65.54,-651.54 72.52,-652.09\"/>\n</g>\n<!-- 5214845680 -->\n<g id=\"node19\" class=\"node\">\n<title>5214845680</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"226,-641.5 149,-641.5 149,-622.5 226,-622.5 226,-641.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-629.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 5214845680&#45;&gt;5214845536 -->\n<g id=\"edge17\" class=\"edge\">\n<title>5214845680&#45;&gt;5214845536</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.5,-622.37C187.5,-613.16 187.5,-598.29 187.5,-586.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"191,-585.91 187.5,-575.91 184,-585.91 191,-585.91\"/>\n</g>\n<!-- 5214845776 -->\n<g id=\"node20\" class=\"node\">\n<title>5214845776</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-707.5 143,-707.5 143,-688.5 244,-688.5 244,-707.5\"/>\n<text text-anchor=\"middle\" x=\"193.5\" y=\"-695.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214845776&#45;&gt;5214845680 -->\n<g id=\"edge18\" class=\"edge\">\n<title>5214845776&#45;&gt;5214845680</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.69,-688.37C191.82,-679.07 190.4,-663.98 189.27,-651.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"192.75,-651.53 188.33,-641.91 185.78,-652.19 192.75,-651.53\"/>\n</g>\n<!-- 5213691152 -->\n<g id=\"node21\" class=\"node\">\n<title>5213691152</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"259,-779 128,-779 128,-749 259,-749 259,-779\"/>\n<text text-anchor=\"middle\" x=\"193.5\" y=\"-767\" font-family=\"monospace\" font-size=\"10.00\">extractor1.0.weight</text>\n<text text-anchor=\"middle\" x=\"193.5\" y=\"-756\" font-family=\"monospace\" font-size=\"10.00\"> (300, 64)</text>\n</g>\n<!-- 5213691152&#45;&gt;5214845776 -->\n<g id=\"edge19\" class=\"edge\">\n<title>5213691152&#45;&gt;5214845776</title>\n<path fill=\"none\" stroke=\"black\" d=\"M193.5,-748.8C193.5,-739.7 193.5,-727.79 193.5,-717.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197,-717.84 193.5,-707.84 190,-717.84 197,-717.84\"/>\n</g>\n<!-- 5214845200 -->\n<g id=\"node22\" class=\"node\">\n<title>5214845200</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"338,-515 261,-515 261,-496 338,-496 338,-515\"/>\n<text text-anchor=\"middle\" x=\"299.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 5214845200&#45;&gt;5214845296 -->\n<g id=\"edge20\" class=\"edge\">\n<title>5214845200&#45;&gt;5214845296</title>\n<path fill=\"none\" stroke=\"black\" d=\"M291.53,-495.87C283.4,-487.05 270.57,-473.16 260.52,-462.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"262.86,-459.64 253.5,-454.67 257.71,-464.39 262.86,-459.64\"/>\n</g>\n<!-- 5214845824 -->\n<g id=\"node23\" class=\"node\">\n<title>5214845824</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"357,-575.5 256,-575.5 256,-556.5 357,-556.5 357,-575.5\"/>\n<text text-anchor=\"middle\" x=\"306.5\" y=\"-563.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214845824&#45;&gt;5214845200 -->\n<g id=\"edge21\" class=\"edge\">\n<title>5214845824&#45;&gt;5214845200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M305.47,-556.37C304.5,-548.25 303.01,-535.81 301.76,-525.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"305.2,-524.68 300.54,-515.17 298.25,-525.51 305.2,-524.68\"/>\n</g>\n<!-- 5213574384 -->\n<g id=\"node24\" class=\"node\">\n<title>5213574384</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"375,-647 244,-647 244,-617 375,-617 375,-647\"/>\n<text text-anchor=\"middle\" x=\"309.5\" y=\"-635\" font-family=\"monospace\" font-size=\"10.00\">extractor1.2.weight</text>\n<text text-anchor=\"middle\" x=\"309.5\" y=\"-624\" font-family=\"monospace\" font-size=\"10.00\"> (100, 300)</text>\n</g>\n<!-- 5213574384&#45;&gt;5214845824 -->\n<g id=\"edge22\" class=\"edge\">\n<title>5213574384&#45;&gt;5214845824</title>\n<path fill=\"none\" stroke=\"black\" d=\"M308.83,-616.8C308.41,-607.7 307.85,-595.79 307.39,-585.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"310.88,-585.67 306.91,-575.84 303.89,-586 310.88,-585.67\"/>\n</g>\n<!-- 5214845104 -->\n<g id=\"node25\" class=\"node\">\n<title>5214845104</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"482,-388.5 387,-388.5 387,-369.5 482,-369.5 482,-388.5\"/>\n<text text-anchor=\"middle\" x=\"434.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 5214845104&#45;&gt;5214844912 -->\n<g id=\"edge23\" class=\"edge\">\n<title>5214845104&#45;&gt;5214844912</title>\n<path fill=\"none\" stroke=\"black\" d=\"M426.81,-369.37C418.02,-359.5 403.43,-343.11 392.39,-330.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"394.7,-328.05 385.43,-322.91 389.47,-332.7 394.7,-328.05\"/>\n</g>\n<!-- 5214845632 -->\n<g id=\"node26\" class=\"node\">\n<title>5214845632</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"485,-454.5 384,-454.5 384,-435.5 485,-435.5 485,-454.5\"/>\n<text text-anchor=\"middle\" x=\"434.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 5214845632&#45;&gt;5214845104 -->\n<g id=\"edge24\" class=\"edge\">\n<title>5214845632&#45;&gt;5214845104</title>\n<path fill=\"none\" stroke=\"black\" d=\"M434.5,-435.37C434.5,-426.16 434.5,-411.29 434.5,-399.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"438,-398.91 434.5,-388.91 431,-398.91 438,-398.91\"/>\n</g>\n<!-- 5214845968 -->\n<g id=\"node27\" class=\"node\">\n<title>5214845968</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"485,-515 384,-515 384,-496 485,-496 485,-515\"/>\n<text text-anchor=\"middle\" x=\"434.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214845968&#45;&gt;5214845632 -->\n<g id=\"edge25\" class=\"edge\">\n<title>5214845968&#45;&gt;5214845632</title>\n<path fill=\"none\" stroke=\"black\" d=\"M434.5,-495.87C434.5,-487.75 434.5,-475.31 434.5,-464.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"438,-464.67 434.5,-454.67 431,-464.67 438,-464.67\"/>\n</g>\n<!-- 5213691392 -->\n<g id=\"node28\" class=\"node\">\n<title>5213691392</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"494,-581 375,-581 375,-551 494,-551 494,-581\"/>\n<text text-anchor=\"middle\" x=\"434.5\" y=\"-569\" font-family=\"monospace\" font-size=\"10.00\">extractor2.0.bias</text>\n<text text-anchor=\"middle\" x=\"434.5\" y=\"-558\" font-family=\"monospace\" font-size=\"10.00\"> (200)</text>\n</g>\n<!-- 5213691392&#45;&gt;5214845968 -->\n<g id=\"edge26\" class=\"edge\">\n<title>5213691392&#45;&gt;5214845968</title>\n<path fill=\"none\" stroke=\"black\" d=\"M434.5,-550.84C434.5,-543.21 434.5,-533.7 434.5,-525.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"438,-525.27 434.5,-515.27 431,-525.27 438,-525.27\"/>\n</g>\n<!-- 5214845920 -->\n<g id=\"node29\" class=\"node\">\n<title>5214845920</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"590,-515 513,-515 513,-496 590,-496 590,-515\"/>\n<text text-anchor=\"middle\" x=\"551.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 5214845920&#45;&gt;5214845632 -->\n<g id=\"edge27\" class=\"edge\">\n<title>5214845920&#45;&gt;5214845632</title>\n<path fill=\"none\" stroke=\"black\" d=\"M534.49,-495.99C515.08,-486.29 483.11,-470.31 460.56,-459.03\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"462.04,-455.86 451.53,-454.52 458.91,-462.12 462.04,-455.86\"/>\n</g>\n<!-- 5214845584 -->\n<g id=\"node30\" class=\"node\">\n<title>5214845584</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"613,-575.5 512,-575.5 512,-556.5 613,-556.5 613,-575.5\"/>\n<text text-anchor=\"middle\" x=\"562.5\" y=\"-563.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214845584&#45;&gt;5214845920 -->\n<g id=\"edge28\" class=\"edge\">\n<title>5214845584&#45;&gt;5214845920</title>\n<path fill=\"none\" stroke=\"black\" d=\"M560.88,-556.37C559.33,-548.16 556.96,-535.54 554.99,-525.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"558.42,-524.35 553.13,-515.17 551.54,-525.64 558.42,-524.35\"/>\n</g>\n<!-- 5213552528 -->\n<g id=\"node31\" class=\"node\">\n<title>5213552528</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"628,-647 497,-647 497,-617 628,-617 628,-647\"/>\n<text text-anchor=\"middle\" x=\"562.5\" y=\"-635\" font-family=\"monospace\" font-size=\"10.00\">extractor2.0.weight</text>\n<text text-anchor=\"middle\" x=\"562.5\" y=\"-624\" font-family=\"monospace\" font-size=\"10.00\"> (200, 64)</text>\n</g>\n<!-- 5213552528&#45;&gt;5214845584 -->\n<g id=\"edge29\" class=\"edge\">\n<title>5213552528&#45;&gt;5214845584</title>\n<path fill=\"none\" stroke=\"black\" d=\"M562.5,-616.8C562.5,-607.7 562.5,-595.79 562.5,-585.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"566,-585.84 562.5,-575.84 559,-585.84 566,-585.84\"/>\n</g>\n<!-- 5214844864 -->\n<g id=\"node32\" class=\"node\">\n<title>5214844864</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"517,-322.5 440,-322.5 440,-303.5 517,-303.5 517,-322.5\"/>\n<text text-anchor=\"middle\" x=\"478.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 5214844864&#45;&gt;5214844768 -->\n<g id=\"edge30\" class=\"edge\">\n<title>5214844864&#45;&gt;5214844768</title>\n<path fill=\"none\" stroke=\"black\" d=\"M468.39,-303.37C456.49,-293.21 436.5,-276.16 421.85,-263.65\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"423.81,-260.73 413.94,-256.91 419.27,-266.06 423.81,-260.73\"/>\n</g>\n<!-- 5214845248 -->\n<g id=\"node33\" class=\"node\">\n<title>5214845248</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"610,-388.5 509,-388.5 509,-369.5 610,-369.5 610,-388.5\"/>\n<text text-anchor=\"middle\" x=\"559.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214845248&#45;&gt;5214844864 -->\n<g id=\"edge31\" class=\"edge\">\n<title>5214845248&#45;&gt;5214844864</title>\n<path fill=\"none\" stroke=\"black\" d=\"M548.58,-369.37C535.53,-359.06 513.49,-341.65 497.6,-329.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"499.53,-326.15 489.51,-322.7 495.19,-331.64 499.53,-326.15\"/>\n</g>\n<!-- 5213552048 -->\n<g id=\"node34\" class=\"node\">\n<title>5213552048</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"634,-460 503,-460 503,-430 634,-430 634,-460\"/>\n<text text-anchor=\"middle\" x=\"568.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\">classifier.0.weight</text>\n<text text-anchor=\"middle\" x=\"568.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\"> (100, 300)</text>\n</g>\n<!-- 5213552048&#45;&gt;5214845248 -->\n<g id=\"edge32\" class=\"edge\">\n<title>5213552048&#45;&gt;5214845248</title>\n<path fill=\"none\" stroke=\"black\" d=\"M566.5,-429.8C565.22,-420.7 563.55,-408.79 562.16,-398.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"565.6,-398.26 560.74,-388.84 558.67,-399.23 565.6,-398.26\"/>\n</g>\n<!-- 5214844576 -->\n<g id=\"node35\" class=\"node\">\n<title>5214844576</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"579,-196 502,-196 502,-177 579,-177 579,-196\"/>\n<text text-anchor=\"middle\" x=\"540.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 5214844576&#45;&gt;5214844720 -->\n<g id=\"edge33\" class=\"edge\">\n<title>5214844576&#45;&gt;5214844720</title>\n<path fill=\"none\" stroke=\"black\" d=\"M518.49,-176.98C496.05,-168.3 461.02,-154.75 435.51,-144.88\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"436.5,-141.51 425.91,-141.17 433.98,-148.04 436.5,-141.51\"/>\n</g>\n<!-- 5214845488 -->\n<g id=\"node36\" class=\"node\">\n<title>5214845488</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"625,-256.5 524,-256.5 524,-237.5 625,-237.5 625,-256.5\"/>\n<text text-anchor=\"middle\" x=\"574.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 5214845488&#45;&gt;5214844576 -->\n<g id=\"edge34\" class=\"edge\">\n<title>5214845488&#45;&gt;5214844576</title>\n<path fill=\"none\" stroke=\"black\" d=\"M569.48,-237.37C564.56,-228.9 556.91,-215.74 550.71,-205.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"553.59,-203.05 545.54,-196.17 547.54,-206.57 553.59,-203.05\"/>\n</g>\n<!-- 5213691552 -->\n<g id=\"node37\" class=\"node\">\n<title>5213691552</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"666,-328 535,-328 535,-298 666,-298 666,-328\"/>\n<text text-anchor=\"middle\" x=\"600.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">classifier.2.weight</text>\n<text text-anchor=\"middle\" x=\"600.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (10, 100)</text>\n</g>\n<!-- 5213691552&#45;&gt;5214845488 -->\n<g id=\"edge35\" class=\"edge\">\n<title>5213691552&#45;&gt;5214845488</title>\n<path fill=\"none\" stroke=\"black\" d=\"M594.73,-297.8C590.95,-288.5 585.99,-276.27 581.92,-266.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"585.1,-264.79 578.09,-256.84 578.61,-267.43 585.1,-264.79\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x136d43610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "x = torch.randn(1, 64)     # dummy input\n",
    "make_dot(model(x), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanila PyTorch Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn # neural network module\n",
    "import torch.nn.functional as F # neural network functional(relu, softmax 등 activation function) module\n",
    "import torch.optim as optim # optimizer(adam 등) module\n",
    "import torchvision # computer vision module\n",
    "import torchvision.transforms as transforms # image preprocessing, augmentation module\n",
    "import matplotlib.pyplot as plt # visualization module\n",
    "import numpy as np # linear algebra module\n",
    "\n",
    "from torchvision.datasets import MNIST # MNIST dataset module\n",
    "from torch.utils.data import DataLoader, Dataset, random_split # data loader module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108044c50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed = 1004\n",
    "random.seed(seed)\n",
    "th.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cuda 사용 가능하면 cuda 사용(only for Windows)\n",
    "# if th.cuda.is_available():\n",
    "#     th.cuda.manual_seed(seed)\n",
    "#     device = th.device('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), # ToTensor(): numpy array(image)를 tensor로 변환\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))]) # Normalize(): image normalization(평균, 표준편차)\n",
    "\n",
    "train_dataset = MNIST(root = './data', train = True, download = True, transform = transform) # train dataset\n",
    "test_dataset = MNIST(root = './data', train = False, download = True, transform = transform) # test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: ./data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "            ),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "            ))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 6000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = random_split(train_dataset, [54000, 6000]) # train dataset을 train dataset과 validation dataset으로 나눔\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_sample = 5\n",
    "random_idx = np.random.randint(0, len(train_dataset), num_sample)\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, idx in enumerate(random_idx):\n",
    "    img, label = train_dataset[idx]\n",
    "    plt.subplot(1, num_sample, i+1)\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "    plt.axis(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader (by batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # batch size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = True) # train dataset을 batch size만큼 나눔\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, drop_last = True) # drop_last: 마지막 batch를 버리고 다시 처음부터 batch size만큼 나눔\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, drop_last = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # self.flatten = nn.Flatten(start_dim = 2, end_dim = 3) # Flatten(): 2차원으로 변환\n",
    "        self.fc1 = nn.Linear(28*28, 256) # fully connected layer\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x) # x = x.view(-1, 28*28)로 대체 가능\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        out = F.log_softmax(x, dim = 1) # log_softmax(): log를 취한 softmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.flatten = nn.Flatten(start_dim = 2, end_dim = 3) # Flatten(): 2차원으로 변환\n",
    "#         self.fc1 = nn.Linear(28*28, 256) # fully connected layer\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "#         self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x) # x = x.view(-1, 28*28)로 대체 가능\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         out = F.log_softmax(x, dim = 1) # log_softmax(): log를 취한 softmax\n",
    "#         return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu') # cuda 사용 가능하면 cuda 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device) # cuda 사용 가능하면 cuda 사용\n",
    "        optimizer.zero_grad() # optimizer gradient 초기화\n",
    "        output = model(data) # model에 data를 넣어서 output 반환\n",
    "        loss = F.nll_loss(output, target) # negative log likelihood loss(nll) 계산\n",
    "        train_losses.append(loss.item()) # loss(scalar(0차원 tensor) -> float)를 list에 append\n",
    "        \n",
    "        loss.backward() # loss를 backpropagation(gradient 계산된 tensor를 각 layer에 저장)\n",
    "        optimizer.step() # optimizer gradient update(optimizer에 저장된 gradient를 각 node의 weight에 적용)\n",
    "        \n",
    "        if batch_idx % 100 == 0: # 100번째 batch마다 loss 출력\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "            \n",
    "    return np.mean(train_losses) # epoch마다 train loss의 평균 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval() # model evaluation에서 안 쓰는 layer들을 끔\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with th.no_grad(): # with 안에서는 gradient 계산 안함\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device) # cuda 사용 가능하면 cuda 사용\n",
    "            output = model(data) # model에 data를 넣어서 output 반환\n",
    "            test_loss += F.nll_loss(output, target, reduction = 'sum').item() # output(logit) 받아서 loss 계산\n",
    "            pred = output.argmax(dim = 1, keepdim = True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() # correct 예측 개수(pred.eq(): tensor의 element-wise 비교)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset) # test loss의 평균\n",
    "        \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset)}%)\\n')\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def run(model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        optimizer,\n",
    "        num_epoch):\n",
    "    \n",
    "    device = th.device('cuda' if th.cuda.is_available() else 'cpu') # cuda 사용 가능하면 cuda 사용\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7) # learning rate scheduler(optim.lr에 있는 scheduler)\n",
    "    model = model.to(device)\n",
    "    train_losses = [] # train loss 저장\n",
    "    val_losses = [] # validation loss 저장\n",
    "    \n",
    "    best_model = None\n",
    "    min_val_loss = np.inf\n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        train_loss = train(model, train_loader, optimizer, epoch, device)\n",
    "        val_loss = test(model, val_loader, device)\n",
    "        scheduler.step() # learning rate scheduler step\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss \n",
    "            best_model = copy.deepcopy(model) # deep copy\n",
    "\n",
    "    th.save(best_model.state_dict(), 'mnist.pt')\n",
    "\n",
    "    test_loss = test(best_model, test_loader, device)\n",
    "\n",
    "    return best_model, train_losses, val_losses, test_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 2.304229\n",
      "Train Epoch: 1 [6400/54000 (12%)]\tLoss: 0.152449\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.302747\n",
      "Train Epoch: 1 [19200/54000 (36%)]\tLoss: 0.376179\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.168665\n",
      "Train Epoch: 1 [32000/54000 (59%)]\tLoss: 0.190544\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.150422\n",
      "Train Epoch: 1 [44800/54000 (83%)]\tLoss: 0.112038\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.074718\n",
      "\n",
      "Test set: Average loss: 0.1238, Accuracy: 5731/6000 (95.51666666666667%)\n",
      "\n",
      "Train Epoch: 2 [0/54000 (0%)]\tLoss: 0.178586\n",
      "Train Epoch: 2 [6400/54000 (12%)]\tLoss: 0.013479\n",
      "Train Epoch: 2 [12800/54000 (24%)]\tLoss: 0.102260\n",
      "Train Epoch: 2 [19200/54000 (36%)]\tLoss: 0.131258\n",
      "Train Epoch: 2 [25600/54000 (47%)]\tLoss: 0.203414\n",
      "Train Epoch: 2 [32000/54000 (59%)]\tLoss: 0.007547\n",
      "Train Epoch: 2 [38400/54000 (71%)]\tLoss: 0.031213\n",
      "Train Epoch: 2 [44800/54000 (83%)]\tLoss: 0.122919\n",
      "Train Epoch: 2 [51200/54000 (95%)]\tLoss: 0.089201\n",
      "\n",
      "Test set: Average loss: 0.0733, Accuracy: 5822/6000 (97.03333333333333%)\n",
      "\n",
      "Train Epoch: 3 [0/54000 (0%)]\tLoss: 0.048169\n",
      "Train Epoch: 3 [6400/54000 (12%)]\tLoss: 0.160352\n",
      "Train Epoch: 3 [12800/54000 (24%)]\tLoss: 0.025737\n",
      "Train Epoch: 3 [19200/54000 (36%)]\tLoss: 0.031168\n",
      "Train Epoch: 3 [25600/54000 (47%)]\tLoss: 0.013953\n",
      "Train Epoch: 3 [32000/54000 (59%)]\tLoss: 0.032142\n",
      "Train Epoch: 3 [38400/54000 (71%)]\tLoss: 0.024842\n",
      "Train Epoch: 3 [44800/54000 (83%)]\tLoss: 0.007355\n",
      "Train Epoch: 3 [51200/54000 (95%)]\tLoss: 0.006739\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 5820/6000 (97.0%)\n",
      "\n",
      "Train Epoch: 4 [0/54000 (0%)]\tLoss: 0.009139\n",
      "Train Epoch: 4 [6400/54000 (12%)]\tLoss: 0.006292\n",
      "Train Epoch: 4 [12800/54000 (24%)]\tLoss: 0.026620\n",
      "Train Epoch: 4 [19200/54000 (36%)]\tLoss: 0.027134\n",
      "Train Epoch: 4 [25600/54000 (47%)]\tLoss: 0.060433\n",
      "Train Epoch: 4 [32000/54000 (59%)]\tLoss: 0.003962\n",
      "Train Epoch: 4 [38400/54000 (71%)]\tLoss: 0.012050\n",
      "Train Epoch: 4 [44800/54000 (83%)]\tLoss: 0.039531\n",
      "Train Epoch: 4 [51200/54000 (95%)]\tLoss: 0.031802\n",
      "\n",
      "Test set: Average loss: 0.0628, Accuracy: 5846/6000 (97.43333333333334%)\n",
      "\n",
      "Train Epoch: 5 [0/54000 (0%)]\tLoss: 0.002558\n",
      "Train Epoch: 5 [6400/54000 (12%)]\tLoss: 0.008328\n",
      "Train Epoch: 5 [12800/54000 (24%)]\tLoss: 0.001681\n",
      "Train Epoch: 5 [19200/54000 (36%)]\tLoss: 0.025555\n",
      "Train Epoch: 5 [25600/54000 (47%)]\tLoss: 0.003717\n",
      "Train Epoch: 5 [32000/54000 (59%)]\tLoss: 0.013488\n",
      "Train Epoch: 5 [38400/54000 (71%)]\tLoss: 0.001603\n",
      "Train Epoch: 5 [44800/54000 (83%)]\tLoss: 0.004510\n",
      "Train Epoch: 5 [51200/54000 (95%)]\tLoss: 0.017174\n",
      "\n",
      "Test set: Average loss: 0.0631, Accuracy: 5850/6000 (97.5%)\n",
      "\n",
      "Train Epoch: 6 [0/54000 (0%)]\tLoss: 0.001128\n",
      "Train Epoch: 6 [6400/54000 (12%)]\tLoss: 0.020743\n",
      "Train Epoch: 6 [12800/54000 (24%)]\tLoss: 0.010335\n",
      "Train Epoch: 6 [19200/54000 (36%)]\tLoss: 0.004004\n",
      "Train Epoch: 6 [25600/54000 (47%)]\tLoss: 0.003265\n",
      "Train Epoch: 6 [32000/54000 (59%)]\tLoss: 0.002855\n",
      "Train Epoch: 6 [38400/54000 (71%)]\tLoss: 0.006501\n",
      "Train Epoch: 6 [44800/54000 (83%)]\tLoss: 0.003820\n",
      "Train Epoch: 6 [51200/54000 (95%)]\tLoss: 0.000595\n",
      "\n",
      "Test set: Average loss: 0.0618, Accuracy: 5851/6000 (97.51666666666667%)\n",
      "\n",
      "Train Epoch: 7 [0/54000 (0%)]\tLoss: 0.012313\n",
      "Train Epoch: 7 [6400/54000 (12%)]\tLoss: 0.002222\n",
      "Train Epoch: 7 [12800/54000 (24%)]\tLoss: 0.002982\n",
      "Train Epoch: 7 [19200/54000 (36%)]\tLoss: 0.001758\n",
      "Train Epoch: 7 [25600/54000 (47%)]\tLoss: 0.000496\n",
      "Train Epoch: 7 [32000/54000 (59%)]\tLoss: 0.004828\n",
      "Train Epoch: 7 [38400/54000 (71%)]\tLoss: 0.001737\n",
      "Train Epoch: 7 [44800/54000 (83%)]\tLoss: 0.002907\n",
      "Train Epoch: 7 [51200/54000 (95%)]\tLoss: 0.003970\n",
      "\n",
      "Test set: Average loss: 0.0619, Accuracy: 5850/6000 (97.5%)\n",
      "\n",
      "Train Epoch: 8 [0/54000 (0%)]\tLoss: 0.000866\n",
      "Train Epoch: 8 [6400/54000 (12%)]\tLoss: 0.004112\n",
      "Train Epoch: 8 [12800/54000 (24%)]\tLoss: 0.002174\n",
      "Train Epoch: 8 [19200/54000 (36%)]\tLoss: 0.000209\n",
      "Train Epoch: 8 [25600/54000 (47%)]\tLoss: 0.001925\n",
      "Train Epoch: 8 [32000/54000 (59%)]\tLoss: 0.000084\n",
      "Train Epoch: 8 [38400/54000 (71%)]\tLoss: 0.003551\n",
      "Train Epoch: 8 [44800/54000 (83%)]\tLoss: 0.002281\n",
      "Train Epoch: 8 [51200/54000 (95%)]\tLoss: 0.000669\n",
      "\n",
      "Test set: Average loss: 0.0635, Accuracy: 5857/6000 (97.61666666666666%)\n",
      "\n",
      "Train Epoch: 9 [0/54000 (0%)]\tLoss: 0.000848\n",
      "Train Epoch: 9 [6400/54000 (12%)]\tLoss: 0.001349\n",
      "Train Epoch: 9 [12800/54000 (24%)]\tLoss: 0.000909\n",
      "Train Epoch: 9 [19200/54000 (36%)]\tLoss: 0.000977\n",
      "Train Epoch: 9 [25600/54000 (47%)]\tLoss: 0.005572\n",
      "Train Epoch: 9 [32000/54000 (59%)]\tLoss: 0.001843\n",
      "Train Epoch: 9 [38400/54000 (71%)]\tLoss: 0.013268\n",
      "Train Epoch: 9 [44800/54000 (83%)]\tLoss: 0.001381\n",
      "Train Epoch: 9 [51200/54000 (95%)]\tLoss: 0.005629\n",
      "\n",
      "Test set: Average loss: 0.0653, Accuracy: 5848/6000 (97.46666666666667%)\n",
      "\n",
      "Train Epoch: 10 [0/54000 (0%)]\tLoss: 0.000451\n",
      "Train Epoch: 10 [6400/54000 (12%)]\tLoss: 0.001180\n",
      "Train Epoch: 10 [12800/54000 (24%)]\tLoss: 0.008272\n",
      "Train Epoch: 10 [19200/54000 (36%)]\tLoss: 0.000993\n",
      "Train Epoch: 10 [25600/54000 (47%)]\tLoss: 0.001296\n",
      "Train Epoch: 10 [32000/54000 (59%)]\tLoss: 0.003574\n",
      "Train Epoch: 10 [38400/54000 (71%)]\tLoss: 0.002815\n",
      "Train Epoch: 10 [44800/54000 (83%)]\tLoss: 0.000337\n",
      "Train Epoch: 10 [51200/54000 (95%)]\tLoss: 0.001257\n",
      "\n",
      "Test set: Average loss: 0.0642, Accuracy: 5853/6000 (97.55%)\n",
      "\n",
      "Train Epoch: 11 [0/54000 (0%)]\tLoss: 0.000987\n",
      "Train Epoch: 11 [6400/54000 (12%)]\tLoss: 0.001730\n",
      "Train Epoch: 11 [12800/54000 (24%)]\tLoss: 0.004244\n",
      "Train Epoch: 11 [19200/54000 (36%)]\tLoss: 0.000893\n",
      "Train Epoch: 11 [25600/54000 (47%)]\tLoss: 0.000704\n",
      "Train Epoch: 11 [32000/54000 (59%)]\tLoss: 0.002721\n",
      "Train Epoch: 11 [38400/54000 (71%)]\tLoss: 0.005190\n",
      "Train Epoch: 11 [44800/54000 (83%)]\tLoss: 0.004570\n",
      "Train Epoch: 11 [51200/54000 (95%)]\tLoss: 0.003798\n",
      "\n",
      "Test set: Average loss: 0.0650, Accuracy: 5851/6000 (97.51666666666667%)\n",
      "\n",
      "Train Epoch: 12 [0/54000 (0%)]\tLoss: 0.000993\n",
      "Train Epoch: 12 [6400/54000 (12%)]\tLoss: 0.001251\n",
      "Train Epoch: 12 [12800/54000 (24%)]\tLoss: 0.002786\n",
      "Train Epoch: 12 [19200/54000 (36%)]\tLoss: 0.000987\n",
      "Train Epoch: 12 [25600/54000 (47%)]\tLoss: 0.000701\n",
      "Train Epoch: 12 [32000/54000 (59%)]\tLoss: 0.000775\n",
      "Train Epoch: 12 [38400/54000 (71%)]\tLoss: 0.001242\n",
      "Train Epoch: 12 [44800/54000 (83%)]\tLoss: 0.006093\n",
      "Train Epoch: 12 [51200/54000 (95%)]\tLoss: 0.001591\n",
      "\n",
      "Test set: Average loss: 0.0648, Accuracy: 5851/6000 (97.51666666666667%)\n",
      "\n",
      "Train Epoch: 13 [0/54000 (0%)]\tLoss: 0.003133\n",
      "Train Epoch: 13 [6400/54000 (12%)]\tLoss: 0.001175\n",
      "Train Epoch: 13 [12800/54000 (24%)]\tLoss: 0.000833\n",
      "Train Epoch: 13 [19200/54000 (36%)]\tLoss: 0.001686\n",
      "Train Epoch: 13 [25600/54000 (47%)]\tLoss: 0.003785\n",
      "Train Epoch: 13 [32000/54000 (59%)]\tLoss: 0.000931\n",
      "Train Epoch: 13 [38400/54000 (71%)]\tLoss: 0.002943\n",
      "Train Epoch: 13 [44800/54000 (83%)]\tLoss: 0.030514\n",
      "Train Epoch: 13 [51200/54000 (95%)]\tLoss: 0.000773\n",
      "\n",
      "Test set: Average loss: 0.0651, Accuracy: 5855/6000 (97.58333333333333%)\n",
      "\n",
      "Train Epoch: 14 [0/54000 (0%)]\tLoss: 0.001188\n",
      "Train Epoch: 14 [6400/54000 (12%)]\tLoss: 0.000628\n",
      "Train Epoch: 14 [12800/54000 (24%)]\tLoss: 0.000383\n",
      "Train Epoch: 14 [19200/54000 (36%)]\tLoss: 0.000895\n",
      "Train Epoch: 14 [25600/54000 (47%)]\tLoss: 0.000690\n",
      "Train Epoch: 14 [32000/54000 (59%)]\tLoss: 0.000576\n",
      "Train Epoch: 14 [38400/54000 (71%)]\tLoss: 0.001449\n",
      "Train Epoch: 14 [44800/54000 (83%)]\tLoss: 0.000249\n",
      "Train Epoch: 14 [51200/54000 (95%)]\tLoss: 0.002074\n",
      "\n",
      "Test set: Average loss: 0.0651, Accuracy: 5852/6000 (97.53333333333333%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0633, Accuracy: 9818/10000 (98.18%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr = 1.0) # optimizer(optim에 있는 optimizer)\n",
    "history = run(model, train_loader, val_loader, test_loader, optimizer, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (15, 5))\n",
    "ax[0].plot(history[1], label = 'train loss')\n",
    "ax[1].plot(history[2], label = 'val loss')\n",
    "ax[0].set_title('train loss')\n",
    "ax[1].set_title('val loss')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[0].grid()\n",
    "ax[1].grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0633, Accuracy: 9818/10000 (98.18%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06327913996970747"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net() # model 생성\n",
    "model.load_state_dict(th.load('mnist.pt')) # best model 불러오기\n",
    "test(model, test_loader, th.device('cuda' if th.cuda.is_available() else 'cpu')) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Test set: Average loss: 0.0633, Accuracy: 9818/10000 (98.18%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06327913996970747"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = history[0] # history[0]에 best model 저장\n",
    "print(best_model)\n",
    "test(best_model, test_loader, th.device('cuda' if th.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ys = best_model(th.stack([test_dataset[i][0] for i in range(len(test_dataset))]).to(device)).argmax(dim = 1).cpu().numpy() # test dataset의 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9834"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_ys == np.array([test_dataset[i][1] for i in range(len(test_dataset))])).sum() / len(test_dataset) # test dataset의 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_dataset[1004][0].squeeze(), cmap='gray')\n",
    "plt.title(f'Label: {pred_ys[1004]}')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy # Accuracy 계산해주는 metric\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer # LightningModule: pytorch model을 wrapping해주는 class\n",
    "from pytorch_lightning.callbacks.progress.tqdm_progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', '.*does not have many workers.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# find DATASET_PATH from environment variable, if not, use './data'\n",
    "DATASET_PATH = os.environ.get('DATASET_PATH', './data')\n",
    "BATCH_SIZE = 256 if th.cuda.is_available() else 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNISTModel(LightningModule):\n",
    "    def __init__(self, \n",
    "                data_dir: str = DATASET_PATH, # data_dir: dataset이 저장되어 있는 경로 (loader에서 사용하기 위해서)\n",
    "                hidden_dim: int = 128, # hidden_dim: hidden layer의 dimension\n",
    "                learning_rate: float = 2e-4):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.num_classes = 10\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.input_dims = (1, 28, 28) # input image의 dimension\n",
    "        self.transform = transforms.Compose([ # transform: image를 어떻게 변환할지 정의\n",
    "            transforms.ToTensor(), # image를 tensor로 변환\n",
    "            transforms.Normalize((0.1307,), (0.3081,)) # image를 normalize(평균, 표준편차)\n",
    "        ])\n",
    "        \n",
    "        channels, width, height = self.input_dims # input image의 dimension을 각각 channels, width, height로 저장\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels * width * height, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, self.num_classes),\n",
    "        )\n",
    "        \n",
    "        self.val_acc = Accuracy(task='multiclass', # val_acc: validation accuracy를 계산하기 위한 metric, multiclass classification을 위한 metric, num_classes: class의 개수\n",
    "                                num_classes=self.num_classes)\n",
    "        self.test_acc = Accuracy(task='multiclass',\n",
    "                                num_classes=self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        out = F.log_softmax(x, dim=1)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch # batch: (x, y) tuple, 데이터 불러오기\n",
    "        logits = self(x) # == self.forward(x), output 지정\n",
    "        loss = F.nll_loss(logits, y) # loss function 지정\n",
    "        self.log('train_loss', loss) # log: tensorboard에 loss를 기록\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = th.argmax(logits, dim=1)\n",
    "        self.val_acc.update(preds, y) # update: metric(valudation acc)을 update\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = th.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        \n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', self.test_acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = th.optim.Adam(self.parameters(), lr=self.learning_rate) # optimizer(Adam) 지정\n",
    "        return optimizer\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(DATASET_PATH, train=True, download=True)\n",
    "        MNIST(DATASET_PATH, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None): # stage: fit, test, predict 중 하나\n",
    "        if stage == 'fit' or stage is None: # fit: train, validation set 만들기\n",
    "            mnist_train = MNIST(DATASET_PATH, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
    "            \n",
    "        if stage == 'test' or stage is None: # test: test set 만들기\n",
    "            self.mnist_test = MNIST(DATASET_PATH, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | model    | Sequential         | 269 K \n",
      "1 | val_acc  | MulticlassAccuracy | 0     \n",
      "2 | test_acc | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "269 K     Trainable params\n",
      "0         Non-trainable params\n",
      "269 K     Total params\n",
      "1.077     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e59c41ba10494786cf5bce07ed1ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_unique2' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m LitMNISTModel(hidden_dim\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m2e-4\u001b[39m)\n\u001b[1;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(accelerator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m# mps: multi-processing strategy, multi-gpu를 사용할 때 사용(for mac), \u001b[39;00m\n\u001b[1;32m      4\u001b[0m                 devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m                 max_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m      6\u001b[0m                 callbacks\u001b[39m=\u001b[39m[TQDMProgressBar(refresh_rate\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)],\n\u001b[1;32m      7\u001b[0m                 logger\u001b[39m=\u001b[39mTensorBoardLogger(\u001b[39m'\u001b[39m\u001b[39mlightning_logs\u001b[39m\u001b[39m'\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmnist\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mmodel)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1103\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1103\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1105\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1182\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1182\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1195\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1194\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1197\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1267\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1267\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1271\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    138\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1485\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1485\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1487\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m, in \u001b[0;36mLitMNISTModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     50\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(logits, y)\n\u001b[1;32m     51\u001b[0m preds \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39margmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_acc\u001b[39m.\u001b[39;49mupdate(preds, y) \u001b[39m# update: metric(valudation acc)을 update\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, loss, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_acc, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/torchmetrics/metric.py:399\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n\u001b[1;32m    392\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    393\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m device corresponds to the device of the input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_on_cpu:\n\u001b[1;32m    402\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/torchmetrics/metric.py:389\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad):\n\u001b[1;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    390\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    391\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/torchmetrics/classification/stat_scores.py:315\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 315\u001b[0m     _multiclass_stat_scores_tensor_validation(\n\u001b[1;32m    316\u001b[0m         preds, target, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_classes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultidim_average, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m preds, target \u001b[39m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_k)\n\u001b[1;32m    319\u001b[0m tp, fp, tn, fn \u001b[39m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[1;32m    320\u001b[0m     preds, target, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_k, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maverage, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultidim_average, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index\n\u001b[1;32m    321\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/torchmetrics/functional/classification/stat_scores.py:303\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    299\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEither `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m and `preds` should be (N, C, ...).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     )\n\u001b[0;32m--> 303\u001b[0m num_unique_values \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(torch\u001b[39m.\u001b[39;49munique(target))\n\u001b[1;32m    304\u001b[0m \u001b[39mif\u001b[39;00m ignore_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     check \u001b[39m=\u001b[39m num_unique_values \u001b[39m>\u001b[39m num_classes\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/torch/_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/torch/_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/torch/functional.py:877\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m    875\u001b[0m     \u001b[39mreturn\u001b[39;00m _unique_impl(\u001b[39minput\u001b[39m, \u001b[39msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 877\u001b[0m output, _, _ \u001b[39m=\u001b[39m _unique_impl(\u001b[39minput\u001b[39;49m, \u001b[39msorted\u001b[39;49m, return_inverse, return_counts, dim)\n\u001b[1;32m    878\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/basicstudy/lib/python3.8/site-packages/torch/functional.py:791\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    783\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39munique_dim(\n\u001b[1;32m    784\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    785\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m         return_counts\u001b[39m=\u001b[39mreturn_counts,\n\u001b[1;32m    789\u001b[0m     )\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 791\u001b[0m     output, inverse_indices, counts \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_unique2(\n\u001b[1;32m    792\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    793\u001b[0m         \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m,\n\u001b[1;32m    794\u001b[0m         return_inverse\u001b[39m=\u001b[39;49mreturn_inverse,\n\u001b[1;32m    795\u001b[0m         return_counts\u001b[39m=\u001b[39;49mreturn_counts,\n\u001b[1;32m    796\u001b[0m     )\n\u001b[1;32m    797\u001b[0m \u001b[39mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_unique2' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "model = LitMNISTModel(hidden_dim=256, learning_rate=2e-4)\n",
    "\n",
    "trainer = Trainer(accelerator='mps', # mps: multi-processing strategy, multi-gpu를 사용할 때 사용(for mac), \n",
    "                devices=1,\n",
    "                max_epochs=5,\n",
    "                callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "                logger=TensorBoardLogger('lightning_logs', name='mnist'))\n",
    "\n",
    "trainer.fit(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basicstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9860fa0492e540ab156d86493a07ae08252682abc63881db291e8eb6322c310b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
